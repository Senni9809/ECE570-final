# -*- coding: utf-8 -*-
"""BIOT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QtLkAyYlFHHSChAk8plmCrkwvqFIlhJ2
"""

from google.colab import drive
drive.mount('/content/drive')

# simplified BIOT-like encoder and training script

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, random_split
from sklearn.metrics import accuracy_score, roc_auc_score
import matplotlib.pyplot as plt
import pickle
import numpy as np
import os

# Load the dataset
import pickle

file_path = '/content/drive/MyDrive/Colab Notebooks/Purdue Coursework/ECE570_Project/trainval.pkl'

with open(file_path, 'rb') as f:
    data = pickle.load(f)

X = data['data']
y = data['labels']

print(X.shape)
print(y.shape)

# Dataset definition
class CHBMITDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.float32)

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

# Split into train/test
from sklearn.model_selection import train_test_split

# 1. Stratified split using sklearn
X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42)

# 2. Wrap into PyTorch datasets
train_ds = CHBMITDataset(X_train, y_train)
val_ds = CHBMITDataset(X_val, y_val)

# 3. Then use your existing dataloaders as-is
train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)
val_loader = DataLoader(val_ds, batch_size=16)

# Define a simplified version of the BIOT encoder
class SimpleBIOT(nn.Module):
    def __init__(self, input_dim=23, seq_len=1280):
        super().__init__()
        self.conv = nn.Conv1d(input_dim, 64, kernel_size=5, padding=2)
        self.relu = nn.ReLU()
        self.pool = nn.AdaptiveAvgPool1d(64)  # reduce to 64 tokens
        self.token_proj = nn.Linear(64, 64)  # token embedding

        encoder_layer = nn.TransformerEncoderLayer(
            d_model=64, nhead=4, dim_feedforward=512, batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=2)
        self.classifier = nn.Sequential(
          nn.Linear(64, 64),
          nn.ReLU(),
          nn.Dropout(0.3),
          nn.Linear(64, 32),
          nn.ReLU(),
          nn.Linear(32, 1)
        )

    def forward(self, x):  # x: (B, C, T)
        x = self.conv(x)
        x = self.relu(x)
        x = self.pool(x)  # (B, 64, 64)
        x = x.permute(0, 2, 1)  # (B, seq, feature) = (B, 64, 64)
        x = self.token_proj(x)
        x = self.transformer(x)
        x = x.mean(dim=1)  # global average pooling
        return self.classifier(x).squeeze(-1)

# Initialize model
model = SimpleBIOT()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
criterion = nn.BCEWithLogitsLoss()

from sklearn.metrics import f1_score
train_loss_list, val_acc_list, val_auc_list = [], [], []

for epoch in range(15):
    model.train()
    total_loss = 0
    for xb, yb in train_loader:
        xb = xb * 1e5
        logits = model(xb)
        loss = criterion(logits, yb)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    # Validation
    model.eval()
    all_probs, all_labels = [], []
    with torch.no_grad():
        for xb, yb in val_loader:
            xb = xb * 1e5
            logits = model(xb)
            probs = torch.sigmoid(logits).cpu().numpy()
            all_probs.extend(probs)
            all_labels.extend(yb.numpy())

    pred_labels = [1 if p > 0.5 else 0 for p in all_probs]
    acc = accuracy_score(all_labels, pred_labels)
    auc = roc_auc_score(all_labels, all_probs)
    f1 = f1_score(all_labels, pred_labels)
    train_loss_list.append(total_loss / len(train_loader))
    val_acc_list.append(acc)
    val_auc_list.append(auc)

    print(f"Epoch {epoch+1}: loss={train_loss_list[-1]:.4f}, acc={acc:.4f}, AUROC={auc:.4f}, F1={f1:.4f}")

import matplotlib.pyplot as plt

epochs = range(1, len(train_loss_list) + 1)

plt.figure(figsize=(12, 4))

# Loss
plt.subplot(1, 3, 1)
plt.plot(epochs, train_loss_list, label="Training Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Training Loss")

# Accuracy
plt.subplot(1, 3, 2)
plt.plot(epochs, val_acc_list, label="Validation Accuracy", color="green")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.title("Validation Accuracy")

# AUROC
plt.subplot(1, 3, 3)
plt.plot(epochs, val_auc_list, label="Validation AUROC", color="orange")
plt.xlabel("Epoch")
plt.ylabel("AUROC")
plt.title("Validation AUROC")

plt.tight_layout()
plt.show()

import pickle

with open("/content/drive/MyDrive/Colab Notebooks/Purdue Coursework/ECE570_Project/test.pkl", "rb") as f:
    test_data = pickle.load(f)

X_test = test_data["data"]       # shape: (N, C, T)
y_test = test_data["labels"]     # shape: (N,)

import torch
from torch.utils.data import TensorDataset, DataLoader

X_test_tensor = torch.tensor(X_test * 1e5, dtype=torch.float32)  # Scale
y_test_tensor = torch.tensor(y_test, dtype=torch.float32)

test_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=32, shuffle=False)

from sklearn.metrics import accuracy_score, roc_auc_score, f1_score

model.eval()
all_probs, all_labels = [], []

with torch.no_grad():
    for xb, yb in test_loader:
        logits = model(xb)
        probs = torch.sigmoid(logits).cpu().numpy()
        all_probs.extend(probs)
        all_labels.extend(yb.numpy())

pred_labels = [1 if p > 0.5 else 0 for p in all_probs]

acc = accuracy_score(all_labels, pred_labels)
auc = roc_auc_score(all_labels, all_probs)
f1 = f1_score(all_labels, pred_labels)

print(f"Test Accuracy: {acc:.4f}")
print(f"Test AUROC:    {auc:.4f}")
print(f"Test F1 Score: {f1:.4f}")

from sklearn.metrics import roc_curve, roc_auc_score

fpr, tpr, thresholds = roc_curve(all_labels, all_probs)
auroc_value = roc_auc_score(all_labels, all_probs)  # 안전하게 별도 변수에 저장

plt.figure(figsize=(6, 5))
plt.plot(fpr, tpr, label=f"ROC Curve (AUROC = {auroc_value:.2f})")
plt.plot([0, 1], [0, 1], linestyle="--", color="gray")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve (Test Set)")
plt.legend()
plt.grid(True)
plt.show()

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

cm = confusion_matrix(all_labels, pred_labels)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["Non-seizure", "Seizure"])

plt.figure(figsize=(5, 5))
disp.plot(cmap="Blues", values_format='d')
plt.title("Confusion Matrix (Test Set)")
plt.show()

from sklearn.metrics import precision_recall_curve, average_precision_score

precision, recall, _ = precision_recall_curve(all_labels, all_probs)
avg_precision = average_precision_score(all_labels, all_probs)

plt.figure(figsize=(6, 5))
plt.plot(recall, precision, label=f"PR Curve (AP = {avg_precision:.2f})", color="darkorange")
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Precision-Recall Curve (Test Set)")
plt.legend()
plt.grid(True)
plt.show()

import numpy as np

# Convert to numpy
all_probs_np = np.array(all_probs)
all_labels_np = np.array(all_labels)

plt.figure(figsize=(6, 5))
plt.hist(all_probs_np[all_labels_np == 0], bins=30, alpha=0.6, label="Non-seizure", color="skyblue")
plt.hist(all_probs_np[all_labels_np == 1], bins=30, alpha=0.6, label="Seizure", color="salmon")
plt.xlabel("Predicted Probability")
plt.ylabel("Count")
plt.title("Histogram of Predicted Probabilities (Test Set)")
plt.legend()
plt.grid(True)
plt.show()

